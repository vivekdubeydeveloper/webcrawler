#here we can define url begin with www as internal url comma(,) seperated
internal.urls=www.prudential.co.uk,www.pru.co.uk

#max no of page you want to crawl if 0 it will completely crawl else crawl till count
max.noofpage.crawl=5

#path of json file where output will be store
output.file.absolute.path=C:\\vivek\\personal\\experiment\\webcrawler\\webcrawlersummary.json

#ignore other than html url
ignore.non.html.format.url=y

#ignore format these can be added comma seperated
ignore.format=pdf,jpg,jpeg,xls,xlsx,zip